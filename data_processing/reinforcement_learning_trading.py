# ğŸš€ PPO(ì •ì±… ìµœì í™”) ê¸°ë°˜ì˜ ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© ì—ì´ì „íŠ¸
# âœ… ê°•í™”í•™ìŠµ íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œ ì£¼ìš” ê¸°ëŠ¥
# âœ” PPO ì•Œê³ ë¦¬ì¦˜ ì ìš©: ì•ˆì •ì ì´ê³  ê°•ë ¥í•œ ìë™ë§¤ë§¤ ìˆ˜í–‰
# âœ” ì‹¤ì‹œê°„ ì‹œì¥ ë°ì´í„° í•™ìŠµ: ê°€ê²©, ë³€ë™ì„±, ê±°ë˜ëŸ‰, ë‰´ìŠ¤ ê°ì„± ë°˜ì˜
# âœ” ìµœì ì˜ ë§¤ë§¤ ì „ëµ í•™ìŠµ: ê°•í™”í•™ìŠµ ê¸°ë°˜ ìë™ë§¤ë§¤ ìµœì í™”
# âœ” AIê°€ ìŠ¤ìŠ¤ë¡œ ì „ëµ ì¡°ì •: ì‹œì¥ ë³€í™”ì— ë”°ë¼ ìœ ì—°í•˜ê²Œ ëŒ€ì‘
# âœ” íƒìƒ‰(Exploration)ê³¼ í™œìš©(Exploitation) ì „ëµ ìµœì í™”

import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from data_processing.time_series_analysis import get_time_series_data
from ai_prediction import predict_price  # AI ì˜ˆì¸¡ ê°€ê²© ê°€ì ¸ì˜¤ê¸°

# ğŸ”¥ 1. íŠ¸ë ˆì´ë”© í™˜ê²½ ì •ì˜
class TradingEnv(gym.Env):
    def __init__(self):
        super(TradingEnv, self).__init__()
        self.ai_predicted_price = predict_price(LSTM_Predictor().cuda(), "price_data.csv")  # AI ê°€ê²© ì˜ˆì¸¡

    def step(self, action):
        # âœ… AI ì˜ˆì¸¡ ê°€ê²©ì„ ê³ ë ¤í•œ ë³´ìƒ í•¨ìˆ˜
        reward = (self.current_price - self.ai_predicted_price) * self.position


class TradingEnv(gym.Env):
    def __init__(self, initial_balance=10000):
        super(TradingEnv, self).__init__()
        self.initial_balance = initial_balance
        self.balance = initial_balance
        self.position = 0  # í˜„ì¬ í¬ì§€ì…˜
        self.data = get_time_series_data()
        self.current_step = 0

        # ìƒíƒœ(State) ê³µê°„: ê°€ê²©, í˜¸ê°€ì°½ ë°ì´í„°, ê±°ë˜ëŸ‰, ë³€ë™ì„± ë“±
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf, shape=(len(self.data[0]),), dtype=np.float32
        )

        # í–‰ë™(Action) ê³µê°„: ë§¤ìˆ˜, ë§¤ë„, ìœ ì§€
        self.action_space = gym.spaces.Discrete(3)

    def step(self, action):
        done = False
        reward = 0
        prev_price = self.data[self.current_step]
        self.current_step += 1
        current_price = self.data[self.current_step]

        if action == 1:  # ë§¤ìˆ˜ (Buy)
            self.position = 1
            self.balance -= current_price
        elif action == 2:  # ë§¤ë„ (Sell)
            self.position = -1
            self.balance += current_price

        # ìˆ˜ìµ ê³„ì‚°
        reward = (current_price - prev_price) * self.position

        # ì¢…ë£Œ ì¡°ê±´
        if self.current_step >= len(self.data) - 1:
            done = True

        return self.data[self.current_step], reward, done, {}

    def reset(self):
        self.current_step = 0
        self.balance = self.initial_balance
        self.position = 0
        return self.data[self.current_step]

# ğŸ”¥ 2. ê°•í™”í•™ìŠµ ëª¨ë¸ í›ˆë ¨
def train_agent():
    env = TradingEnv()
    env = DummyVecEnv([lambda: env])  # Stable-Baselines3 í˜¸í™˜ìš©
    model = PPO("MlpPolicy", env, verbose=1)
    
    print("ğŸš€ ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ í›ˆë ¨ ì‹œì‘...")
    model.learn(total_timesteps=50000)
    print("âœ… í›ˆë ¨ ì™„ë£Œ!")

    model.save("ppo_trading_agent")

# ğŸ”¥ 3. í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸
def test_agent():
    model = PPO.load("ppo_trading_agent")
    env = TradingEnv()
    state = env.reset()
    done = False
    total_profit = 0

    while not done:
        action, _ = model.predict(state)
        state, reward, done, _ = env.step(action)
        total_profit += reward

    print(f"ğŸ¯ ìµœì¢… ìˆ˜ìµ: {total_profit}")

if __name__ == "__main__":
    train_agent()
    test_agent()

# ğŸ“Œ ğŸ’¡ ì¶”ê°€ ê°œì„  ê°€ëŠ¥ í•­ëª© 
# âœ… DDPG, SAC ë“± ë‹¤ë¥¸ RL ì•Œê³ ë¦¬ì¦˜ ì‹¤í—˜
# âœ… ë” ì •êµí•œ ë³´ìƒ í•¨ìˆ˜ ì¶”ê°€
# âœ… ê±°ë˜ëŸ‰ ê¸°ë°˜ ë³€ë™ì„± ì¡°ì •
# âœ… ì¥ê¸° í•™ìŠµì„ í†µí•œ ìµœì ì˜ íŠ¸ë ˆì´ë”© ì „ëµ íƒìƒ‰