import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from data_processing.time_series_analysis import get_time_series_data
from ai_prediction import predict_price  # AI ì˜ˆì¸¡ ê°€ê²© ê°€ì ¸ì˜¤ê¸°

# ğŸ”¥ 1. íŠ¸ë ˆì´ë”© í™˜ê²½ ì •ì˜
class TradingEnv(gym.Env):
    def __init__(self, selected_coins=SELECTED_COINS, initial_balance=10000):
        super(TradingEnv, self).__init__()
        self.selected_coins = selected_coins
        self.initial_balance = initial_balance
        self.balance = initial_balance
        self.position = 0  # í˜„ì¬ í¬ì§€ì…˜
        self.current_step = 0

        # ìƒíƒœ(State) ê³µê°„: ê° ì½”ì¸ì˜ ê°€ê²©, í˜¸ê°€ì°½ ë°ì´í„°, ê±°ë˜ëŸ‰, ë³€ë™ì„± ë“±
        self.data = {symbol: get_time_series_data(symbol) for symbol in selected_coins}
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf, shape=(len(self.data[self.selected_coins[0]][0]),), dtype=np.float32
        )

        # í–‰ë™(Action) ê³µê°„: ë§¤ìˆ˜, ë§¤ë„, ìœ ì§€
        self.action_space = gym.spaces.Discrete(3)

    def step(self, action):
        done = False
        reward = 0
        prev_price = self.data[self.selected_coins[0]][self.current_step]
        self.current_step += 1
        current_price = self.data[self.selected_coins[0]][self.current_step]

        if action == 1:  # ë§¤ìˆ˜ (Buy)
            self.position = 1
            self.balance -= current_price
        elif action == 2:  # ë§¤ë„ (Sell)
            self.position = -1
            self.balance += current_price

        # ìˆ˜ìµ ê³„ì‚°
        reward = (current_price - prev_price) * self.position
        if self.balance <= 0:
            done = True  # ì”ê³ ê°€ 0 ì´í•˜ë¡œ ë–¨ì–´ì§€ë©´ ì¢…ë£Œ

        # ìƒíƒœ ê°±ì‹ 
        state = np.array(self.data[self.selected_coins[0]][self.current_step])

        return state, reward, done, {}

    def reset(self):
        self.balance = self.initial_balance
        self.position = 0
        self.current_step = 0
        return np.array(self.data[self.selected_coins[0]][self.current_step])

# ğŸ”¥ 2. PPO ì•Œê³ ë¦¬ì¦˜ ì ìš©
def train_model():
    env = DummyVecEnv([lambda: TradingEnv()])  # ë²¡í„°í™”ëœ í™˜ê²½ ìƒì„±
    model = PPO("MlpPolicy", env, verbose=1, learning_rate=0.001, n_steps=512, batch_size=64)
    model.learn(total_timesteps=10000)

    # ëª¨ë¸ ì €ì¥
    model.save("ppo_trading_model")
    print("ğŸš€ PPO ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")

    return model

# ğŸ”¥ 3. ëª¨ë¸ í‰ê°€
def evaluate_model(model, env):
    obs = env.reset()
    total_reward = 0
    done = False

    while not done:
        action, _states = model.predict(obs)
        obs, reward, done, info = env.step(action)
        total_reward += reward

    print(f"ì´ ë³´ìƒ: {total_reward}")

# âœ… ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # PPO ëª¨ë¸ í•™ìŠµ
    model = train_model()

    # í•™ìŠµëœ ëª¨ë¸ í‰ê°€
    test_env = TradingEnv(selected_coins=["BTCUSDT", "ETHUSDT"])
    evaluate_model(model, test_env)
